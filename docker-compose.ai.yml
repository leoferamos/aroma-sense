services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.dev
    container_name: aroma-backend-dev
    ports:
      - "8080:8080"
    volumes:
      - ./backend:/app
      - go-modules:/go/pkg/mod
    environment:
      - GIN_MODE=debug
      - CGO_ENABLED=0
      # AI Configuration for local Ollama
      - AI_PROVIDER=ollama
      - AI_LLM_MODEL=tinyllama:latest
      - AI_LLM_BASE_URL=http://ollama:11434
      - AI_EMB_MODEL=nomic-embed-text:latest
      - AI_EMB_BASE_URL=http://ollama:11434
      - AI_TIMEOUT=30
    networks:
      - aroma-network
    restart: unless-stopped
    depends_on:
      - frontend
      - ollama

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    container_name: aroma-frontend-dev
    ports:
      - "5173:5173"
    volumes:
      - ./frontend:/app
    environment:
      - VITE_API_URL=http://backend:8080
      - VITE_APP_ENV=development
    networks:
      - aroma-network
    restart: unless-stopped
  ollama:
    image: ollama/ollama:latest
    container_name: aroma-ollama-dev
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - aroma-network
    restart: unless-stopped
    environment:
      - OLLAMA_NUM_PARALLEL=1
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:11434/api/tags || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
    
volumes:
  go-modules:
    driver: local
  node-modules:
    driver: local
  ollama-data:
    driver: local

# Custom network for service communication
networks:
  aroma-network:
    driver: bridge